------------------------------------------------------------------------------------
Search for Extraterrestrial Intelligence: GPU Accelerated E.T. Search with TurboSETI
------------------------------------------------------------------------------------

.. class:: abstract

A common technique adopted by the SETI community is monitoring electromagnetic radiation for signs of extraterrestrial technosignatures using ground-based radio observatories. 
The analysis is made using a Python-based software called TurboSETI to detect narrowband drifting signals inside the recordings that can mean a technosignature (E.T.).
The data stream generated by a telescope can easily reach the rate of terabits per second. 
Our goal was to improve the processing speeds by writing a GPU-accelerated backend in addition to the original CPU-based implementation of the de-doppler algorithm.
We discuss how we ported a CPU-only program to leverage the parallel capabilities of a GPU using CuPy, Numba, and custom CUDA kernels. 
The accelerated backend reached a speed-up of an order of magnitude over the CPU implementation.

.. class:: keywords

gpu, numba, cupy, seti, turboseti

1. Introduction
----------------

The Search for Extraterrestrial Intelligence (SETI) is a broad term utilized to describe the effort of locating any scientific proof of past or present technology that originated beyond the bounds of Earth. 
SETI can be performed in a plethora of ways: either actively by deploying orbiters and rovers around planets/moons within the solar system, or passively by either searching for biosignatures in exoplanet atmospheres or “listening” to technologically-capable extraterrestrial civilizations.
One of the most common techniques adopted by the SETI community is monitoring electromagnetic radiation for narrowband signs of technosignatures using ground-based radio observatories.
This search can be performed in equipment primarily built for this task, like the Allen Telescope Array (California, USA), renting observation time, or in the background while the primary user is conducting other observations. 
Other radio-observatories useful for this search include the MeerKAT Telescope (Northern Cape, South Africa), Green Bank Telescope (West Virginia, USA), and the Parkes Telescope (New South Wales, Australia).
The operation of a radio-telescope is similar to an optical telescope.
Instead of using optics to concentrate light into an optical sensor, a radio-telescope operates by concentrating electromagnetic waves into an antenna using a large reflective structure called a “dish”.
The interaction between the metallic antenna and the electromagnetic wave generates a faint electrical current.
This effect is then quantized by an analog-to-digital converter as RAW voltages and transmitted to a processing logic to extract useful information from it.
The data stream generated by a radio telescope can easily reach the rate of terabits per second.
The current workflow utilized by the Breakthrough Listen, the largest scientific research program aimed at finding evidence of extraterrestrial intelligence, consists in pre-processing and storing the incoming data as frequency-time binary files (:cite:`lebofsky_croft_siemion_price_enriquez_isaacson_macmahon_anderson_brzycki_cobb_et`) in persistent storage for later analysis. 
This post-analysis is made possible using a Python-based software called TurboSETI (:cite:`enriquez_2019`) to detect narrowband signals that could be drifting in frequency owing to the relative radial velocity between the observer on earth, and the transmitter.
Although being an offline operation, the processing speed of TurboSETI is in lockstep with the amount of science we can extract from each observation. Each filterbank file ingested by TurboSETI is often on the order of a few hundreds of gigabytes. To process data efficiently, the program uses Numpy for near machine-level performance. 
To measure a potential signal's drift rate, TurboSETI uses a de-doppler algorithm to align the frequency axis according to a pre-set drift rate. Another algorithm called “hitsearch” is then utilized to identify any signal present in the recorded spectrum. 
These two algorithms are the most resource-hungry elements of the pipeline consuming almost 90% of the running time.

2. Approach
-----------

Multiple methods were utilized in this effort to write a GPU-accelerated backend and optimize the CPU implementation of TurboSETI. In this section, we enumerate them.

2.1. CuPy
+++++++++

The original implementation of TurboSETI heavily depends on Numpy (:cite:`harris2020array`) for data processing. 
To keep the number of modifications as low as possible, we implemented the GPU-accelerated backend using CuPy (:cite:`cupy_learningsys2017`). 
This open-source library offers GPU acceleration backed by NVIDIA CUDA and AMD ROCm while using a Numpy style API. 
This enabled us to share most of the code between the CPU and GPU-based implementations.

2.1. Numba
++++++++++

Some computationally heavy methods of the original CPU-based implementation of TurboSETI were written in Cython.
This approach has disadvantages: the developer has to be familiar with Cython syntax to alter the code; the code requires additional logic to be compiled at installation time. 
Consequently, it was decided to replace Cython with pure Python methods decorated with the Numba (:cite:`numba_2022`) accelerator. 
By leveraging the power of LLVM’s Just-In-Time compiler, Numba can compile Python code into assembly code as well as apply SIMD acceleration instructions to achieve near machine-level speeds.

2.2. Single-Precision Floating-Point
++++++++++++++++++++++++++++++++++++

The original implementation of the software handled the input data as double-precision floating-point numbers. This behavior would cause all the mathematical operations to take significantly longer to process because of the added precision. The ultimate precision of the output product is inherently limited by the precision of the original input data which in most cases is represented by an 8-bit signed integer. Therefore, the addition of a single-precision floating-point number decreased the processing time without compromising the useful precision of the output data.

3. Results
----------

To test the speed improvements between implementations we used filterbank files from previous observations coming from different observatories. 
Table 1 indicates the processing times it took to process three different files in double-precision mode. 
We can notice that the CPU implementation based on Numba is measurably faster than the original Cython implementation. 
At the same time, the GPU-accelerated backend processed the data from 6.8 to 9.3 times faster than the original CPU-based implementation.

+--------------------------------------------------------------------------------+
| Double-Precision (float64)                                                     |
+================+========+==============+===================+===================+
| Implementation | Device |   Voyager 1  | Voyager 1 (2-bit) | Voyager 1 (8-bit) |
+----------------+--------+--------------+-------------------+-------------------+
| Cython         | CPU    | 0.44 minutes | 25.26 minutes     | 23.06 minutes     |
+----------------+--------+--------------+-------------------+-------------------+
| Numba          | CPU    | 0.36 minutes | 20.67 minutes     | 22.44 minutes     |
+----------------+--------+--------------+-------------------+-------------------+
| CuPy           | GPU    | 0.05 minutes | 2.73 minutes      | 3.40 minutes      |
+----------------+--------+--------------+-------------------+-------------------+

Table 2 indicates the same results as Table 1 but with single-precision floating points. 
The original Cython implementation was left out because it doesn’t support single-precision mode. 
Here, the same data was processed from 7.5 to 10.6 times faster than the Numba CPU-based implementation.

+--------------------------------------------------------------------------------+
| Double-Precision (float64)                                                     |
+================+========+==============+===================+===================+
| Implementation | Device |   Voyager 1  | Voyager 1 (2-bit) | Voyager 1 (8-bit) |
+----------------+--------+--------------+-------------------+-------------------+
| Numba          | CPU    | 0.26 minutes | 16.13 minutes     | 16.15 minutes     |
+----------------+--------+--------------+-------------------+-------------------+
| CuPy           | GPU    | 0.03 minutes | 1.52 minutes      | 2.14 minutes      |
+----------------+--------+--------------+-------------------+-------------------+

To illustrate the processing time improvement, a single observation containing 105 GB of data was processed in 12 hours by the original CPU-based TurboSETI implementation on an i7-7700K Intel CPU, and just 1 hour and 45 minutes by the GPU-accelerated backend on a GTX 1070 Ti NVIDIA GPU.

4. Conclusion
-------------

The original implementation of TurboSETI worked exclusively on the CPU to process data. 
We implemented a GPU-accelerated backend to leverage the massive parallelization capabilities of a graphical device. 
The new implementation resulted in significantly less time to process observation data resulting in more science being produced.