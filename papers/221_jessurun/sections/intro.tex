\section{Introduction}
Labeled image data is essential for tuning and evaluating the performance of machine learning applications.
Such labels are typically defined with approximate enclosing shapes (i.e. simple polygons or parametric shapes), which tend to misrepresent more complex components.
% While this streamlines the labeling process, it misrepresents more complex components.
When high accuracy is required, labels must be specified at or close to the pixel-level â€“ a process known as semantic labeling or semantic segmentation.
A detailed description of this process is given in \cite{chengSurveyAnalysisAutomatic2018}. Examples can readily be found in several popular datasets such as COCO, depicted in \autoref{fig:sampleSegData}.

\makeSampleSegFig

% However, a wide variety of applications require pixel-level accuracy, ranging from hardware assurance to medical imaging.
% Alternatively, semantic segmentation is a technique providing pixel-level accuracy which avoids poor foreground representation~\cite{chengSurveyAnalysisAutomatic2018}.
%One such field applying this method include Bill-of-Material (BoM) extraction. This BoM, or list of all surface-mount devices (SMDs) on a printed circuit board (PCB) surface, can be generated from optical images of a board under test. Next, it can be compared against a reference design to detect likely counterfeit, tampered, or defective SMDs~\cite{paradis2020color,azhaganReviewAutomaticBill2019}. To create such a BoM using optical data alone, it is crucial that detected SMDs use segmentation masks that are representative of structures such as pins, solder flow, and more. This is the only way the collected data will remain indicative of true BoM properties. An example annotation is shown in \autoref{fig:pcb}.

Semantic segmentation is important in numerous domains including printed circuit board assembly (PCBA) inspection (discussed later in Section \ref{sec:case_study}) \cite{paradis2020color,azhaganReviewAutomaticBill2019}, quality control during manufacturing \cite{fergusonDetectionSegmentationManufacturing2018,anagnostopoulosComputerVisionApproach2001,anagnostopoulosHighPerformanceComputing2002}, manuscript restoration / digitization \cite{gatosSegmentationfreeRecognitionTechnique2004,kesimanNewSchemeText2016,jainTextSegmentationUsing1992,taxtSegmentationDocumentImages1989,fujisawaSegmentationMethodsCharacter1992}, and effective patient diagnosis \cite{seifertSemanticAnnotationMedical2010,rajchlDeepCutObjectSegmentation2017,yushkevichUserguided3DActive2006,iakovidisRatsnakeVersatileImage2014}.
In all these cases, imprecise annotations severely limit the development of automated solutions and can decrease the accuracy of standard trained segmentation models.

Quality semantic segmentation is difficult due to a reliance on large, high-quality datasets, which are often created by manually labeling each image.
Manual annotation is error-prone, costly, and greatly hinders scalability. As such, several tools have been proposed to alleviate the burden of collecting these ground-truth labels~\cite{BestImageAnnotation}.
% The more commonly used applications are listed in \cite{BestImageAnnotation}.
Unfortunately, existing tools are heavily biased toward lower-resolution images with few regions of interest (ROI), similar to \autoref{fig:sampleSegData}.
While this may not be an issue for some datasets, such assumptions are \emph{crippling} for high-fidelity images with hundreds of annotated ROIs~\cite{Ladicky_whatWhereCombiningCRFs,Wang_multiLabelImageAnnotation}.
% This scenario is represented in Figure~\ref{fig:bees} but can occur in multiple of the domains previously listed.
% Especially when each region can be arbitrarily complex, the software will enter a non-responsive state where no annotation can be performed.

% A potential workaround is to bootstrap machine learning models through transfer learning on a similar dataset and applying them on the current dataset.
% Manual supervision is then only required to verify the results are correct and make adjustments accordingly.
% While this approach is valid when existing datasets match the desired segmentation properties, it also means transfer learning is ineffective when training on novel data or image properties.
% Moreover, transfer learning is effective in assisting ground truth collection only when a sufficient data repository has already been gathered against which to validate network training \cite{opbroekTransferLearningImproves2015,weissSurveyTransferLearning2016}.

% Even after models are trained, it can be greatly beneficial to explore edge cases and pre/post-processing techniques while supervising the ground truth collection procedure.
%Toward this end,

With improving hardware capabilities and increasing need for high-resolution ground truth segmentation, there are a continually growing number of applications that \textit{require} high-resolution imaging with the previously described characteristics \cite{Mohajerani_cloudRemoteSensing,Demochkina_improvingOneShotXray}.
%[TODO: DESCRIBE 1 OR 2 APPLICATIONS]~\cite{TODO:}.
In these cases, the existing annotation tooling greatly impacts productivity due to the previously referenced assumptions and lack of support \cite{SpaceNet2020-lb}.

In response to these bottlenecks, \emph{we present the Semi-Supervised Semantic Annotation (S3A) annotation and prototyping platform -- an application which eases the process of pixel-level labeling in large, complex scenes.}%
\footnote{A preliminary version was introduced in an earlier publication~\cite{jessurunComponentDetectionEvaluation2020}, but significant changes to the framework and tool capabilities have been employed since then.}
Its graphical user interface is shown in \autoref{fig:appOverview}.
The software includes live app-level property customization, real-time algorithm modification and feedback, region prediction assistance, constrained component table editing based on allowed data types, various data export formats, and a highly adaptable set of plugin interfaces for domain-specific extensions to S3A.
Beyond software improvements, these features play significant roles in bridging the gap between human annotation efforts and scalable, automated segmentation methods \cite{Branson_humansInLoop}.

\makeAppOverviewFig